{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import copy\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import space library and get its vocabularies\n",
    "from spacy.en import English, LOCAL_DATA_DIR\n",
    "data_dir = os.environ.get('SPACY_DATA', LOCAL_DATA_DIR)\n",
    "nlp = English(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the list with reporting verbs and HTML files\n",
    "verbList = pd.read_csv('ReportingVerbList2.csv').values\n",
    "verbs = map(lambda x: x.replace(\" \", \"\"),[item for sublist in verbList for item in sublist])\n",
    "html1 = BeautifulSoup(open('NYTPakDrone2004_2013_1.HTML','r'),'html.parser')\n",
    "html2 = BeautifulSoup(open('NYTPakDrone2004_2013_2.HTML','r'),'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findTextFromHtml(html):\n",
    "    \"\"\" Reads HTML file and returns dict with respect to needed columns in final CSV file \"\"\"\n",
    "    \n",
    "    import datetime\n",
    "    def change_date_format(line):\n",
    "        line = ' '.join(line.replace(',', '').split()[:3])\n",
    "        try:\n",
    "            new_date = datetime.datetime.strptime(line, '%B %d %Y').strftime('%m/%d/%Y')\n",
    "        except:\n",
    "            new_date = ''\n",
    "        return new_date\n",
    "    \n",
    "    # define CSV file's columns.Text of an article we wite to 'ARTICLETEXT'\n",
    "    CSV_columns = ['ARTICLENO', 'PUBLICATION', 'DATE', 'HEADLINE', 'BYLINE', 'SECTION', 'LENGTH', 'DATELINE', \n",
    "                   'URL', 'GRAPHIC', 'LANGUAGE', 'PUBLICATION-TYPE', 'SUBJECT', 'COMPANY', 'ORGANIZATION', \n",
    "                   'PERSON', 'CITY', 'COUNTRY', 'REGION', 'LOAD-DATE', 'ARTICLETEXT']\n",
    "                   \n",
    "    parsedDiv = html.findAll('div')\n",
    "    for div in parsedDiv:\n",
    "        if div.text==\"\":\n",
    "            parsedDiv.remove(div)        \n",
    "    text = map(lambda x: x.text.replace('\\n','').replace('\\t',''), parsedDiv)\n",
    "    \n",
    "    res = []\n",
    "    article_counter = 0\n",
    "    new_art_i = 0\n",
    "    max_len_i = 0\n",
    "    for i in range(len(text)):\n",
    "        len_i = len(text[i])\n",
    "        if 'DOCUMENTS' in text[i]:\n",
    "            new_art_i = i\n",
    "            max_len_i = 0\n",
    "            article_counter += 1\n",
    "            new_article = {key: '' for key in CSV_columns}\n",
    "            new_article['ARTICLENO'] = article_counter\n",
    "            continue\n",
    "        if i-1 == new_art_i:\n",
    "            new_article['PUBLICATION'] = text[i]\n",
    "            continue\n",
    "        if i-2 == new_art_i:\n",
    "            new_article['DATE'] = change_date_format(text[i])\n",
    "            continue\n",
    "        if i-3 == new_art_i:\n",
    "            new_article['HEADLINE'] = text[i]\n",
    "            continue\n",
    "        if i > new_art_i and len_i > max_len_i:\n",
    "            max_len_i = len_i\n",
    "            new_article['ARTICLETEXT'] = text[i]\n",
    "        for j in CSV_columns[CSV_columns.index('BYLINE'):-1]:\n",
    "            if j+':' in text[i]:\n",
    "                new_article[j] = text[i].split(j+':')[1].strip()\n",
    "        if 'Copyright' in text[i]:\n",
    "            res.append(new_article)\n",
    "    return res\n",
    "\n",
    "\n",
    "class IndexedText(object):\n",
    "    \"\"\" Transforms a word to its general form \"\"\"\n",
    "    \n",
    "    def __init__(self, stemmer,lemmatizer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._lemmatizer = lemmatizer\n",
    "        self._index = nltk.Index((self._stem(word), i) for (i, word) in enumerate(text))\n",
    "        #self._verbs = filter(lambda x: 'vb' in x[1].lower(), nltk.pos_tag(text))\n",
    "        #self._stemVerbs = map(lambda x: self._stem(x[0]), self._verbs)\n",
    "\n",
    "    def concordance(self, word):\n",
    "        key = self._stem(word)              # words of context\n",
    "        finded = []\n",
    "        #if key in self._stemVerbs:\n",
    "        if self._index[key]:\n",
    "            context = ' '.join(self._text)\n",
    "            finded.append(context)\n",
    "        return finded\n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._lemmatizer.lemmatize(self._stemmer.stem(word).lower(), pos='v') \n",
    "\n",
    "    \n",
    "def Searcher(texts,verbs):\n",
    "    \"\"\" Searches verbs from REPORTINGVERB_LIST in the sentences \"\"\"\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    porter = nltk.PorterStemmer()\n",
    "    searchRes = []\n",
    "    for senNo,text in enumerate(texts):\n",
    "        tokTest = nltk.word_tokenize(text)\n",
    "        indexText = IndexedText(porter,lemmatizer, tokTest)\n",
    "        for verb in verbs:\n",
    "            con = indexText.concordance(verb)\n",
    "            if con:                \n",
    "                for i in con:\n",
    "                    searchRes.append([senNo+1,i,verb])\n",
    "    return searchRes               \n",
    "\n",
    "\n",
    "def sentenceSplitter(texto,verbs):\n",
    "    \"\"\" Divides text on sentences \"\"\"\n",
    "    \n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(texto.strip())\n",
    "    searcherRes = Searcher(sentences,verbs)\n",
    "    return searcherRes\n",
    "\n",
    "\n",
    "def findFullSubj(fullSubj,tok):\n",
    "    \"\"\" Builds the tree of connection for long sources \"\"\"\n",
    "    \n",
    "    for itl in [t.orth_ for t in tok.lefts]:\n",
    "        fullSubj.insert(fullSubj.index(tok.orth_),itl)\n",
    "    for itr in reversed([t.orth_ for t in tok.rights]):\n",
    "        fullSubj.insert(fullSubj.index(tok.orth_)+1,itr)\n",
    "    for tl in [t for t in tok.lefts]:\n",
    "        findFullSubj(fullSubj,tl)\n",
    "    for tr in [t for t in tok.rights]:\n",
    "        findFullSubj(fullSubj,tr)\n",
    "    \n",
    "\n",
    "def getSource(verbs,searcherRes):\n",
    "    \"\"\" Defines the source for respective verb \"\"\"\n",
    "    \n",
    "    resWithSource = copy.deepcopy(searcherRes)\n",
    "    for ind, result in enumerate(searcherRes):\n",
    "        text = nlp(result[1])\n",
    "        verb = nlp(unicode(result[2]))\n",
    "        for t in verb:verbLemma = t.lemma_\n",
    "        source = None\n",
    "        notVB = True\n",
    "        for token in text:            \n",
    "            if verbLemma == token.lemma_:\n",
    "                if token.tag_ not in ['NN','NNS']:\n",
    "                    notVB = False\n",
    "                else:\n",
    "                    continue\n",
    "                startToken = token\n",
    "                reps=0\n",
    "                while True:\n",
    "                    if reps>10: break\n",
    "                    if (token.dep_==u'nsubj' or token.dep_==u'nsubjpass') and token.head == startToken:\n",
    "                        source = token\n",
    "                        break\n",
    "                    if token.dep_==u'ROOT':\n",
    "                        branches = [t for t in token.lefts]+[t for t in token.rights]\n",
    "                        for branch in branches:\n",
    "                            if branch.dep_== u'nsubj' or branch.dep_==  u'nsubjpass':\n",
    "                                source = branch\n",
    "                                break\n",
    "                        if source:\n",
    "                            break\n",
    "                    token = token.head\n",
    "                    reps+=1\n",
    "        if notVB:\n",
    "            resWithSource[ind].append(u'DELETE')\n",
    "        else:\n",
    "            if source:\n",
    "                if source.tag_ == u'PRP' or source.tag_ == u'DT':\n",
    "                    resWithSource[ind].append(u'NO SOURCE')\n",
    "                else:\n",
    "                    FullSubj=[source.orth_,]\n",
    "                    findFullSubj(FullSubj,source)\n",
    "                    resWithSource[ind].append(' '.join(FullSubj))\n",
    "            else:\n",
    "                resWithSource[ind].append(u'NO SOURCE')\n",
    "    resWithSource = [x for x in resWithSource if x[3] != u'DELETE']\n",
    "    for ind, res in enumerate(resWithSource):\n",
    "        if res[3] == u'NO SOURCE':\n",
    "            for revers in reversed(range(ind)):\n",
    "                if resWithSource[revers][3] != u'NO SOURCE':\n",
    "                    res[3] = resWithSource[revers][3]\n",
    "                    break\n",
    "    return resWithSource\n",
    "\n",
    "\n",
    "def clear_sentence(text):\n",
    "    \"\"\" Additional cleaner to sentences \"\"\"\n",
    "    \n",
    "    while not text[0].isalpha():\n",
    "        text = text[1:]\n",
    "    while not text[-1].isalpha():\n",
    "        text = text[:-1]\n",
    "    text = ','.join(map(lambda x: x.rstrip(), text.split(',')))\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_CSV_for_html(html, saving_file_name):\n",
    "    \"\"\" Creates and saves a CSV file for respective html file \"\"\"\n",
    "    \n",
    "    listo = findTextFromHtml(html)\n",
    "    fullSourceReport=[]\n",
    "    i=0\n",
    "    for texto in listo:\n",
    "        seRes = sentenceSplitter(texto['ARTICLETEXT'],verbs)\n",
    "        teSources = getSource(verbs,seRes)\n",
    "        fullSourceReport.append(teSources)\n",
    "        i+=1\n",
    "        #print i\n",
    "\n",
    "    dfListo = pd.DataFrame.from_dict(listo, orient='columns')\n",
    "    dfListo = dfListo.drop('ARTICLETEXT', 1)\n",
    "\n",
    "    formattedSourceRep=[]\n",
    "    for ind, textRep in enumerate(fullSourceReport):\n",
    "        for row in textRep:\n",
    "            formattedSourceRep.append([ind+1]+row)\n",
    "\n",
    "    dfReport = pd.DataFrame.from_records(formattedSourceRep, columns=['ARTICLENO','SENTENCENO','SENTENCE','REPORTEDVERB','SOURCE'])\n",
    "    dfMerged = dfListo.merge(dfReport, how='right', on='ARTICLENO')\n",
    "    dfMerged['NOWORDS'] = \"\"\n",
    "    dfMerged['SENTENCE'] = dfMerged['SENTENCE'].apply(clear_sentence)\n",
    "    dfMerged.to_csv(saving_file_name, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_CSV_for_html(html1, 'result_html1.csv')\n",
    "    create_CSV_for_html(html2, 'result_html2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for concatenate all CSV for each HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that for correct work the folder containing current IPython Notebook file should \n",
    "# contain only saved CSV files for each HTML file (not other CSV files)  \n",
    "def concatenate_all_CSV(path='./'):\n",
    "    files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path,f)) and f.split('.')[-1] == 'csv']\n",
    "    df = pd.DataFrame()\n",
    "    for f in files:\n",
    "        df_i = pd.read_csv(f)\n",
    "        try:\n",
    "            df = pd.concat([df, df_i])\n",
    "        except:\n",
    "            continue\n",
    "    for i in list(df.columns):\n",
    "        if 'Unnamed' in i:\n",
    "            df.drop(i, axis=1, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv('all.csv')\n",
    "    \n",
    "concatenate_all_CSV()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
